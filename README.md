# Welcome to My Convex Optimization
***

## Task
What is the problem? And where is the challenge?

The problem is to understand and implement convex optimization and gradient descent methods. The challenge lies in finding the minimum of 

a convex function using different optimization algorithms and understanding the impact of learning rate on the efficiency of gradient descent.



## Description
How have you solved the problem?

To s

olve the problem, I have implemented several functions:

print_a_function(f, values): This function plots a given function with the provided values.
find_root_bisection(f, min, max): This function uses the dichotomous algorithm (bisection method) to find the zero of a function within a given range.
find_root_newton_raphson(f, f_deriv): This function uses the Newton-Raphson's method to find the zero of a function.
gradient_descent(f, f_prime, start, learning_rate = 0.1): This function performs gradient descent to find the minimum of a function.
solve_linear_problem(A, b, c): This function solves a linear problem using the simplex method.




## Installation
How to install your project? npm install? make? make re?





## Usage
How does it work?


```
./my_project argument1 argument2
```

### The Core Team


<span><i>Made at <a href='https://qwasar.io'>Qwasar SV -- Software Engineering School</a></i></span>
<span><img alt='Qwasar SV -- Software Engineering School's Logo' src='https://storage.googleapis.com/qwasar-public/qwasar-logo_50x50.png' width='20px'></span>
